{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ea4880",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, time, gc, random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "device = torch.device(\"cuda:0\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b119f1f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(os.path.abspath(os.path.join('..', 'source')))\n",
    "from CountNetTorch import CountNet, seed_everything\n",
    "from metrics import compute_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a091e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_data = os.path.abspath('data')\n",
    "path_res  = os.path.abspath('results')\n",
    "metrics = ['RMSP', 'RMD', 'R2']\n",
    "n_folds = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d4ecf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_names = ['epilepsy', 'cd4', 'bolus', 'owls', 'fruits']\n",
    "optimizer = 'Adam'\n",
    "learning_rates = [0.01, 0.001]\n",
    "batch_sizes = [32, 16]\n",
    "num_nodes_list = [[8,8], [4,4]]\n",
    "activation = torch.nn.LeakyReLU()\n",
    "max_epochs = 200\n",
    "patience = 10\n",
    "sig2u_init = 0.8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a5af16",
   "metadata": {},
   "source": [
    "### Poisson-NN(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d17ed395",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------\n",
    "model_name = 'PNN-M'\n",
    "# ---------------------------------\n",
    "is_conditional = False\n",
    "for learning_rate in learning_rates:\n",
    "    for batch_size in batch_sizes:\n",
    "        for num_nodes in num_nodes_list:\n",
    "            experiment = f'lr{learning_rate}-batch{batch_size}-nodes{num_nodes[0]}'\n",
    "            results = {metric:np.zeros((n_folds, len(data_names))) for metric in metrics+['time']}\n",
    "            for data_number, data_name in enumerate(data_names):\n",
    "                data = pd.read_csv(f'{path_data}/{data_name}-prep.csv', dtype='float32', engine='pyarrow')        \n",
    "                n_clusters = np.unique(data['id']).shape[0]\n",
    "                n_features = 1+max([int(col_name[1:]) if 'x' in col_name else 0 for col_name in data.columns])\n",
    "                for test_fold in tqdm(range(n_folds), desc=data_name):\n",
    "                    valid_fold = (test_fold+1)//n_folds\n",
    "                    data_dict = {\n",
    "                        'train':data[-data['fold'].isin([valid_fold, test_fold])],\n",
    "                        'valid':data[data['fold'] == valid_fold],\n",
    "                        'test' :data[data['fold'] == test_fold],\n",
    "                    }\n",
    "                    for subset_name, subset in data_dict.items():\n",
    "                        z = np.array(subset['id'], dtype=np.int32)\n",
    "                        globals()[f'X_{subset_name}'] = np.array(subset[[f'x{i}' for i in range(n_features)]], dtype=np.float32)\n",
    "                        globals()[f'Z_{subset_name}'] = np.zeros((len(z), n_clusters), dtype=np.float32)\n",
    "                        globals()[f'Z_{subset_name}'][np.arange(len(z)), z] = 1\n",
    "                        globals()[f'y_{subset_name}'] = np.array(subset['y'], dtype=np.float32).reshape(-1,1)\n",
    "                    torch.cuda.empty_cache(); gc.collect()\n",
    "                    seed_everything()\n",
    "                    M = CountNet(n_clusters, n_features, num_nodes, activation, device=device)\n",
    "                    start_time = time.time()\n",
    "                    M.train_clik([y_train, X_train, Z_train], [y_valid, X_valid, Z_valid], is_conditional,\n",
    "                        optimizer, learning_rate, batch_size, max_epochs, patience)\n",
    "                    computing_time = time.time() - start_time\n",
    "                    y_pred = M.predict(X_test)\n",
    "                        offset_test = np.array(data_dict['test']['offset']).flatten()\n",
    "                        y_pred = y_pred * offset_test\n",
    "                        y_test = y_test * offset_test\n",
    "                        y_train = y_train * np.array(data_dict['train']['offset']).flatten()\n",
    "                    temp_results = compute_metrics(y_test, y_pred, np.mean(y_train), metrics)\n",
    "                    for metric in metrics:\n",
    "                        results[metric][test_fold, data_number] = temp_results[metric]\n",
    "                    results['time'][test_fold, data_number] = computing_time\n",
    "                    del M; gc.collect()\n",
    "            for metric in metrics+['time']:\n",
    "                pd.DataFrame(results[metric], columns=data_names).to_csv(f'{path_res}/{model_name}-{metric}-{experiment}.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6faf5801",
   "metadata": {},
   "source": [
    "### Poisson-NN(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e73ec2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------\n",
    "model_name = 'PNN-C'\n",
    "# ---------------------------------\n",
    "results = {metric:np.zeros((n_repeats*len(input_distributions), len(random_effects_types))) for metric in metrics+['time']}\n",
    "experiments = [f'{input_distribution}-{simulation_number}' for input_distribution in input_distributions for simulation_number in range(n_repeats)]\n",
    "is_conditional = True\n",
    "for k_re, random_effects_type in enumerate(random_effects_types):    \n",
    "    for k_in, input_distribution in enumerate(input_distributions):\n",
    "        for simulation_number in tqdm(range(n_repeats), desc=f'X: {input_distribution}, u: {random_effects_type}'):\n",
    "            data = pd.read_csv(f'{path_data}/data-{input_distribution}-{random_effects_type}-{simulation_number}.csv')\n",
    "            data = {\n",
    "                'train':data[data['number'].isin(range(cluster_size_train))],\n",
    "                'valid':data[data['number'].isin(range(cluster_size_train, cluster_size_train+cluster_size_valid))],\n",
    "                'test' :data[data['number'].isin(range(cluster_size_train+cluster_size_valid, cluster_size))],\n",
    "            }\n",
    "            for subset_name, subset in data.items():\n",
    "                z = np.array(subset['cluster'], dtype=np.int32)\n",
    "                globals()[f'y_{subset_name}'] = np.array(subset['y'], dtype=np.float32).reshape(-1,1)\n",
    "                globals()[f'X_{subset_name}'] = np.array(subset[[f'x{i}' for i in range(n_features)]], dtype=np.float32)\n",
    "                globals()[f'Z_{subset_name}'] = np.zeros((len(z), n_clusters), dtype=np.float32)\n",
    "                globals()[f'Z_{subset_name}'][np.arange(len(z)), z] = 1                \n",
    "            torch.cuda.empty_cache(); gc.collect()\n",
    "            seed_everything()\n",
    "            M = CountNet(n_clusters, n_features, num_nodes, activation, device=device)\n",
    "            start_time = time.time()\n",
    "            M.train_clik([y_train, X_train, Z_train], [y_valid, X_valid, Z_valid], is_conditional,\n",
    "                optimizer, learning_rate, batch_size, max_epochs, patience)\n",
    "            computing_time = time.time() - start_time\n",
    "            y_pred = M.predict(X_test, Z_test)            \n",
    "            temp_results = compute_metrics(y_test, y_pred, np.mean(y_train), metrics)\n",
    "            for metric in metrics:\n",
    "                results[metric][(k_in*n_repeats+simulation_number), k_re] = temp_results[metric]\n",
    "            results['time'][(k_in*n_repeats+simulation_number), k_re] = computing_time\n",
    "            del M; gc.collect()    \n",
    "for metric in metrics+['time']:\n",
    "    pd.DataFrame(results[metric], columns=random_effects_types, index=experiments).to_csv(f'{path_res}/{model_name}-{metric}.csv', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ad9482",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------\n",
    "model_name = 'PNN-C'\n",
    "# ---------------------------------\n",
    "is_conditional = True\n",
    "for learning_rate in learning_rates:\n",
    "    for batch_size in batch_sizes:\n",
    "        for num_nodes in num_nodes_list:\n",
    "            experiment = f'lr{learning_rate}-batch{batch_size}-nodes{num_nodes[0]}'\n",
    "            results = {metric:np.zeros((n_folds, len(data_names))) for metric in metrics+['time']}\n",
    "            for data_number, data_name in enumerate(data_names):\n",
    "                data = pd.read_csv(f'{path_data}/{data_name}-prep.csv', dtype='float32', engine='pyarrow')        \n",
    "                n_clusters = np.unique(data['id']).shape[0]\n",
    "                n_features = 1+max([int(col_name[1:]) if 'x' in col_name else 0 for col_name in data.columns])\n",
    "                for test_fold in tqdm(range(n_folds), desc=data_name):\n",
    "                    valid_fold = (test_fold+1)//n_folds\n",
    "                    data_dict = {\n",
    "                        'train':data[-data['fold'].isin([valid_fold, test_fold])],\n",
    "                        'valid':data[data['fold'] == valid_fold],\n",
    "                        'test' :data[data['fold'] == test_fold],\n",
    "                    }\n",
    "                    for subset_name, subset in data_dict.items():\n",
    "                        z = np.array(subset['id'], dtype=np.int32)\n",
    "                        globals()[f'X_{subset_name}'] = np.array(subset[[f'x{i}' for i in range(n_features)]], dtype=np.float32)\n",
    "                        globals()[f'Z_{subset_name}'] = np.zeros((len(z), n_clusters), dtype=np.float32)\n",
    "                        globals()[f'Z_{subset_name}'][np.arange(len(z)), z] = 1\n",
    "                        globals()[f'y_{subset_name}'] = np.array(subset['y'], dtype=np.float32).reshape(-1,1)\n",
    "                    torch.cuda.empty_cache(); gc.collect()\n",
    "                    seed_everything()\n",
    "                    M = CountNet(n_clusters, n_features, num_nodes, activation, device=device)\n",
    "                    start_time = time.time()\n",
    "                    M.train_clik([y_train, X_train, Z_train], [y_valid, X_valid, Z_valid], is_conditional,\n",
    "                        optimizer, learning_rate, batch_size, max_epochs, patience)\n",
    "                    computing_time = time.time() - start_time\n",
    "                    y_pred = M.predict(X_test, Z_test)\n",
    "                    if data_name == 'owls':\n",
    "                        offset_test = np.array(data_dict['test']['offset']).flatten()\n",
    "                        y_pred = y_pred * offset_test\n",
    "                        y_test = y_test * offset_test\n",
    "                        y_train = y_train * np.array(data_dict['train']['offset']).flatten()\n",
    "                    temp_results = compute_metrics(y_test, y_pred, np.mean(y_train), metrics)\n",
    "                    for metric in metrics:\n",
    "                        results[metric][test_fold, data_number] = temp_results[metric]\n",
    "                    results['time'][test_fold, data_number] = computing_time\n",
    "                    del M; gc.collect()\n",
    "            for metric in metrics+['time']:\n",
    "                pd.DataFrame(results[metric], columns=data_names).to_csv(f'{path_res}/{model_name}-{metric}-{experiment}.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "398c7d76",
   "metadata": {},
   "source": [
    "### PGNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a69641c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------\n",
    "model_name = 'PGNN'\n",
    "# ---------------------------------\n",
    "results = {metric:np.zeros((n_repeats*len(input_distributions), len(random_effects_types))) for metric in metrics+['time']}\n",
    "experiments = [f'{input_distribution}-{simulation_number}' for input_distribution in input_distributions for simulation_number in range(n_repeats)]\n",
    "for k_re, random_effects_type in enumerate(random_effects_types):\n",
    "    for k_in, input_distribution in enumerate(input_distributions):\n",
    "        for simulation_number in tqdm(range(n_repeats), desc=f'X: {input_distribution}, u: {random_effects_type}'):\n",
    "            data = pd.read_csv(f'{path_data}/data-{input_distribution}-{random_effects_type}-{simulation_number}.csv')\n",
    "            data = {\n",
    "                'train':data[data['number'].isin(range(cluster_size_train))],\n",
    "                'valid':data[data['number'].isin(range(cluster_size_train, cluster_size_train+cluster_size_valid))],\n",
    "                'test' :data[data['number'].isin(range(cluster_size_train+cluster_size_valid, cluster_size))],\n",
    "            }\n",
    "            for subset_name, subset in data.items():\n",
    "                z = np.array(subset['cluster'], dtype=np.int32)\n",
    "                globals()[f'y_{subset_name}'] = np.array(subset['y'], dtype=np.float32).reshape(-1,1)\n",
    "                globals()[f'X_{subset_name}'] = np.array(subset[[f'x{i}' for i in range(n_features)]], dtype=np.float32)\n",
    "                globals()[f'Z_{subset_name}'] = np.zeros((len(z), n_clusters), dtype=np.float32)\n",
    "                globals()[f'Z_{subset_name}'][np.arange(len(z)), z] = 1                \n",
    "            torch.cuda.empty_cache(); gc.collect()\n",
    "            seed_everything()\n",
    "            M = CountNet(n_clusters, n_features, num_nodes, activation, device=device)\n",
    "            start_time = time.time()\n",
    "            M.train_hlik([y_train, X_train, Z_train], [y_valid, X_valid, Z_valid],\n",
    "                optimizer, learning_rate, batch_size, max_epochs, patience, sig2u_init)\n",
    "            computing_time = time.time() - start_time\n",
    "            y_pred = M.predict(X_test, Z_test)\n",
    "            temp_results = compute_metrics(y_test, y_pred, np.mean(y_train), metrics)\n",
    "            for metric in metrics:\n",
    "                results[metric][(k_in*n_repeats+simulation_number), k_re] = temp_results[metric]\n",
    "            results['time'][(k_in*n_repeats+simulation_number), k_re] = computing_time\n",
    "            file_history = f'{os.path.abspath('model')}/{model_name}-{input_distribution}-{random_effects_type}-{simulation_number}-history.pkl'\n",
    "            with open(file_history, 'wb') as f: pickle.dump(M.history, f)\n",
    "            del M; gc.collect()\n",
    "for metric in metrics+['time']:\n",
    "    pd.DataFrame(results[metric], columns=random_effects_types, index=experiments).to_csv(f'{path_res}/{model_name}-{metric}.csv', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "083a3df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------\n",
    "model_name = 'HLMMNN'\n",
    "# ---------------------------------\n",
    "for learning_rate in learning_rates:\n",
    "    for batch_size in batch_sizes:\n",
    "        for num_nodes in num_nodes_list:\n",
    "            experiment = f'lr{learning_rate}-batch{batch_size}-nodes{num_nodes[0]}'\n",
    "            results = {metric:np.zeros((n_folds, len(data_names))) for metric in metrics+['time']}\n",
    "            for data_number, data_name in enumerate(data_names):\n",
    "                data = pd.read_csv(f'{path_data}/{data_name}-prep.csv', dtype='float32', engine='pyarrow')        \n",
    "                n_clusters = np.unique(data['id']).shape[0]\n",
    "                n_features = 1+max([int(col_name[1:]) if 'x' in col_name else 0 for col_name in data.columns])\n",
    "                for test_fold in tqdm(range(n_folds), desc=data_name):\n",
    "                    valid_fold = (test_fold+1)//n_folds\n",
    "                    data_dict = {\n",
    "                        'train':data[-data['fold'].isin([valid_fold, test_fold])],\n",
    "                        'valid':data[data['fold'] == valid_fold],\n",
    "                        'test' :data[data['fold'] == test_fold],\n",
    "                    }\n",
    "                    for subset_name, subset in data_dict.items():\n",
    "                        z = np.array(subset['id'], dtype=np.int32)\n",
    "                        globals()[f'X_{subset_name}'] = np.array(subset[[f'x{i}' for i in range(n_features)]], dtype=np.float32)\n",
    "                        globals()[f'Z_{subset_name}'] = np.zeros((len(z), n_clusters), dtype=np.float32)\n",
    "                        globals()[f'Z_{subset_name}'][np.arange(len(z)), z] = 1\n",
    "                        globals()[f'y_{subset_name}'] = np.array(subset['y'], dtype=np.float32).reshape(-1,1)\n",
    "                    torch.cuda.empty_cache(); gc.collect()\n",
    "                    seed_everything()\n",
    "                    M = CountNet(n_clusters, n_features, num_nodes, activation, device=device)\n",
    "                    start_time = time.time()\n",
    "                    M.train_hlik([y_train, X_train, Z_train], [y_valid, X_valid, Z_valid],\n",
    "                        optimizer, learning_rate, batch_size, max_epochs, patience, sig2u_init)\n",
    "                    computing_time = time.time() - start_time\n",
    "                    y_pred = M.predict(X_test, Z_test)\n",
    "                    if data_name == 'owls':\n",
    "                        offset_test = np.array(data_dict['test']['offset']).flatten()\n",
    "                        y_pred = y_pred * offset_test\n",
    "                        y_test = y_test * offset_test\n",
    "                        y_train = y_train * np.array(data_dict['train']['offset']).flatten()\n",
    "                    temp_results = compute_metrics(y_test, y_pred, np.mean(y_train), metrics)\n",
    "                    for metric in metrics:\n",
    "                        results[metric][test_fold, data_number] = temp_results[metric]\n",
    "                    results['time'][test_fold, data_number] = computing_time\n",
    "                    del M; gc.collect()\n",
    "            for metric in metrics+['time']:\n",
    "                pd.DataFrame(results[metric], columns=data_names).to_csv(f'{path_res}/{model_name}-{metric}-{experiment}.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
